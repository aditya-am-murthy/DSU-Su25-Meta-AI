 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Extraction\n",
    "\n",
    "In this notebook, we'll learn how to extract meaningful features from video frames using pre-trained neural networks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how pre-trained models work for feature extraction\n",
    "- Extract features from video frames using different architectures\n",
    "- Compare different feature extraction methods\n",
    "- Understand temporal pooling strategies\n",
    "- **Complete 6 hands-on exercises** requiring deep understanding\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Feature Extraction**: Converting raw video frames into numerical representations that capture important visual information.\n",
    "\n",
    "**Pre-trained Models**: Neural networks trained on large datasets (like ImageNet) that can extract useful features from images.\n",
    "\n",
    "**Temporal Pooling**: Combining features from multiple frames to create a single video representation.\n",
    "\n",
    "## Important Note\n",
    "\n",
    "This notebook contains **interactive exercises** that require you to understand the underlying concepts. Each exercise builds on previous knowledge and requires critical thinking about feature extraction strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our utilities\n",
    "from utils.video_utils import load_video, create_frame_transforms, frames_to_tensor\n",
    "from utils.model_utils import VideoFeatureExtractor, get_model_summary\n",
    "from utils.data_utils import VideoDataset\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Pre-trained Models\n",
    "\n",
    "Let's start by exploring what pre-trained models are available and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available pre-trained models\n",
    "print(\"Available pre-trained models in torchvision:\")\n",
    "print(\"\\nResNet models:\")\n",
    "print(\"- resnet18\")\n",
    "print(\"- resnet34\")\n",
    "print(\"- resnet50\")\n",
    "print(\"- resnet101\")\n",
    "print(\"- resnet152\")\n",
    "\n",
    "print(\"\\nOther models:\")\n",
    "print(\"- vgg16\")\n",
    "print(\"- alexnet\")\n",
    "print(\"- squeezenet\")\n",
    "print(\"- densenet121\")\n",
    "\n",
    "# Let's examine the structure of ResNet50\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESNET50 ARCHITECTURE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "print(f\"ResNet50 has {len(list(resnet50.children()))} main layers:\")\n",
    "\n",
    "for i, layer in enumerate(resnet50.children()):\n",
    "    print(f\"Layer {i}: {type(layer).__name__}\")\n",
    "    if hasattr(layer, 'out_channels'):\n",
    "        print(f\"  Output channels: {layer.out_channels}\")\n",
    "    elif hasattr(layer, 'out_features'):\n",
    "        print(f\"  Output features: {layer.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 1: Model Architecture Analysis\n",
    "\n",
    "**Task**: Analyze different pre-trained models and understand their characteristics.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load 3 different pre-trained models (ResNet18, VGG16, DenseNet121)\n",
    "2. Compare their parameter counts and model sizes\n",
    "3. Analyze the output feature dimensions of each model\n",
    "4. Create a bar chart comparing model complexities\n",
    "5. Determine which model would be best for real-time processing\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your model architecture analysis code\n",
    "\n",
    "# 1. Load 3 different models\n",
    "# Your code here...\n",
    "\n",
    "# 2. Compare parameter counts\n",
    "# Your code here...\n",
    "\n",
    "# 3. Analyze output dimensions\n",
    "# Your code here...\n",
    "\n",
    "# 4. Create comparison chart\n",
    "# Your code here...\n",
    "\n",
    "# 5. Determine best for real-time processing\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Video Data\n",
    "\n",
    "Let's load some video data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = project_root / \"data\" / \"videos\"\n",
    "metadata_file = data_dir / \"sample_metadata.csv\"\n",
    "\n",
    "# Load a sample video\n",
    "video_files = list(data_dir.glob(\"*.mp4\"))\n",
    "if video_files:\n",
    "    sample_video = str(video_files[0])\n",
    "    print(f\"Loading video: {os.path.basename(sample_video)}\")\n",
    "    \n",
    "    # Load video frames\n",
    "    frames = load_video(sample_video, max_frames=10)  # Use fewer frames for demonstration\n",
    "    print(f\"Loaded {len(frames)} frames with shape: {frames.shape}\")\n",
    "    \n",
    "    # Show first few frames\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(10, len(frames))):\n",
    "        axes[i].imshow(frames[i])\n",
    "        axes[i].set_title(f'Frame {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No video files found. Please run the data download script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Feature Extraction\n",
    "\n",
    "Let's manually extract features from a single frame to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual feature extraction from a single frame\n",
    "if 'frames' in locals():\n",
    "    # Take the first frame\n",
    "    single_frame = frames[0]\n",
    "    print(f\"Single frame shape: {single_frame.shape}\")\n",
    "    \n",
    "    # Create transforms for the frame\n",
    "    transform = create_frame_transforms(image_size=224, is_training=False)\n",
    "    \n",
    "    # Transform the frame\n",
    "    frame_tensor = transform(single_frame)\n",
    "    print(f\"Transformed frame tensor shape: {frame_tensor.shape}\")\n",
    "    \n",
    "    # Add batch dimension\n",
    "    frame_batch = frame_tensor.unsqueeze(0)\n",
    "    print(f\"Batch tensor shape: {frame_batch.shape}\")\n",
    "    \n",
    "    # Extract features using ResNet50 (remove the final classification layer)\n",
    "    resnet50_features = models.resnet50(pretrained=True)\n",
    "    # Remove the final classification layer\n",
    "    feature_extractor = nn.Sequential(*list(resnet50_features.children())[:-1])\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(frame_batch)\n",
    "        \n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    print(f\"Feature vector length: {features.numel()}\")\n",
    "    \n",
    "    # Flatten the features\n",
    "    features_flat = features.squeeze()\n",
    "    print(f\"Flattened features shape: {features_flat.shape}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(f\"Mean: {features_flat.mean():.4f}\")\n",
    "    print(f\"Std: {features_flat.std():.4f}\")\n",
    "    print(f\"Min: {features_flat.min():.4f}\")\n",
    "    print(f\"Max: {features_flat.max():.4f}\")\n",
    "    \n",
    "    # Visualize feature distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(features_flat.numpy(), bins=50, alpha=0.7)\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Feature Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(features_flat.numpy()[:100])  # Show first 100 features\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.title('First 100 Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(features_flat.numpy()[-100:])  # Show last 100 features\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.title('Last 100 Features')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 2: Feature Analysis and Comparison\n",
    "\n",
    "**Task**: Compare features extracted from different frames and models.\n",
    "\n",
    "**Requirements**:\n",
    "1. Extract features from 3 different frames of the same video\n",
    "2. Calculate the cosine similarity between these feature vectors\n",
    "3. Extract features using 2 different models (ResNet18 and VGG16)\n",
    "4. Compare the feature distributions between models\n",
    "5. Create a heatmap showing feature similarities across frames\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your feature analysis and comparison code\n",
    "\n",
    "# 1. Extract features from 3 different frames\n",
    "# Your code here...\n",
    "\n",
    "# 2. Calculate cosine similarity\n",
    "# Your code here...\n",
    "\n",
    "# 3. Extract features with different models\n",
    "# Your code here...\n",
    "\n",
    "# 4. Compare feature distributions\n",
    "# Your code here...\n",
    "\n",
    "# 5. Create similarity heatmap\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Our VideoFeatureExtractor Class\n",
    "\n",
    "Now let's use our custom VideoFeatureExtractor class to extract features from multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extractor\n",
    "feature_extractor = VideoFeatureExtractor(\n",
    "    model_name='resnet50',\n",
    "    image_size=224,\n",
    "    pooling_strategy='mean'\n",
    ")\n",
    "\n",
    "print(\"Feature extractor created successfully!\")\n",
    "print(f\"Model: {feature_extractor.model_name}\")\n",
    "print(f\"Output dimension: {feature_extractor.output_dim}\")\n",
    "print(f\"Pooling strategy: {feature_extractor.pooling_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from video frames\n",
    "if 'frames' in locals():\n",
    "    print(f\"Extracting features from {len(frames)} frames...\")\n",
    "    \n",
    "    # Extract features\n",
    "    video_features = feature_extractor.extract_features(frames)\n",
    "    \n",
    "    print(f\"Video features shape: {video_features.shape}\")\n",
    "    print(f\"Feature vector length: {video_features.numel()}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(f\"Mean: {video_features.mean():.4f}\")\n",
    "    print(f\"Std: {video_features.std():.4f}\")\n",
    "    print(f\"Min: {video_features.min():.4f}\")\n",
    "    print(f\"Max: {video_features.max():.4f}\")\n",
    "    \n",
    "    # Visualize feature vector\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(video_features.numpy(), bins=50, alpha=0.7)\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Video Feature Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(video_features.numpy())\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.title('Video Feature Vector')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 3: Temporal Pooling Strategies\n",
    "\n",
    "**Task**: Implement and compare different temporal pooling strategies.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement 3 different pooling strategies: mean, max, and attention-based\n",
    "2. Extract frame-level features from a video\n",
    "3. Apply each pooling strategy to get video-level features\n",
    "4. Compare the resulting feature vectors\n",
    "5. Analyze which pooling strategy preserves the most information\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your temporal pooling strategies code\n",
    "\n",
    "# 1. Implement pooling strategies\n",
    "# Your code here...\n",
    "\n",
    "# 2. Extract frame-level features\n",
    "# Your code here...\n",
    "\n",
    "# 3. Apply each pooling strategy\n",
    "# Your code here...\n",
    "\n",
    "# 4. Compare feature vectors\n",
    "# Your code here...\n",
    "\n",
    "# 5. Analyze information preservation\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Multiple Videos\n",
    "\n",
    "Let's extract features from multiple videos efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple videos\n",
    "if video_files:\n",
    "    print(f\"Processing {min(5, len(video_files))} videos...\")\n",
    "    \n",
    "    all_video_features = []\n",
    "    video_names = []\n",
    "    \n",
    "    for i, video_file in enumerate(video_files[:5]):\n",
    "        try:\n",
    "            print(f\"Processing video {i+1}: {video_file.name}\")\n",
    "            \n",
    "            # Load frames\n",
    "            frames = load_video(str(video_file), max_frames=10)\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extractor.extract_features(frames)\n",
    "            \n",
    "            all_video_features.append(features)\n",
    "            video_names.append(video_file.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file.name}: {e}\")\n",
    "    \n",
    "    if all_video_features:\n",
    "        # Stack all features\n",
    "        features_matrix = torch.stack(all_video_features)\n",
    "        print(f\"\\nFeatures matrix shape: {features_matrix.shape}\")\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        similarities = torch.mm(features_matrix, features_matrix.t())\n",
    "        \n",
    "        # Normalize similarities\n",
    "        norms = torch.norm(features_matrix, dim=1, keepdim=True)\n",
    "        normalized_similarities = similarities / (norms * norms.t())\n",
    "        \n",
    "        # Visualize similarity matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(normalized_similarities.numpy(), cmap='viridis')\n",
    "        plt.colorbar(label='Cosine Similarity')\n",
    "        plt.title('Video Similarity Matrix')\n",
    "        plt.xlabel('Video Index')\n",
    "        plt.ylabel('Video Index')\n",
    "        \n",
    "        # Add video names as tick labels\n",
    "        plt.xticks(range(len(video_names)), [name[:15] + '...' for name in video_names], rotation=45)\n",
    "        plt.yticks(range(len(video_names)), [name[:15] + '...' for name in video_names])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 4: Feature Similarity Analysis\n",
    "\n",
    "**Task**: Analyze feature similarities and identify patterns.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load metadata to get video labels\n",
    "2. Calculate average similarity within same-label videos vs different-label videos\n",
    "3. Create a confusion matrix showing similarity patterns\n",
    "4. Identify which videos are most similar/different\n",
    "5. Suggest feature extraction improvements based on your analysis\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your feature similarity analysis code\n",
    "\n",
    "# 1. Load metadata and get labels\n",
    "# Your code here...\n",
    "\n",
    "# 2. Calculate average similarities\n",
    "# Your code here...\n",
    "\n",
    "# 3. Create confusion matrix\n",
    "# Your code here...\n",
    "\n",
    "# 4. Identify most similar/different videos\n",
    "# Your code here...\n",
    "\n",
    "# 5. Suggest improvements\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Dimensionality and Compression\n",
    "\n",
    "Let's explore techniques for reducing feature dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dimensionality reduction techniques\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "if 'features_matrix' in locals():\n",
    "    print(f\"Original feature dimension: {features_matrix.shape[1]}\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_matrix.numpy())\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=50)  # Reduce to 50 dimensions\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    print(f\"PCA reduced dimension: {features_pca.shape[1]}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    # Visualize explained variance\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(features_pca[:, 0], features_pca[:, 1])\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('PCA Visualization')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 5: Dimensionality Reduction Analysis\n",
    "\n",
    "**Task**: Compare different dimensionality reduction techniques.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement PCA, t-SNE, and UMAP for dimensionality reduction\n",
    "2. Compare the quality of reduced features using reconstruction error\n",
    "3. Visualize the reduced features in 2D\n",
    "4. Analyze which technique preserves similarity relationships best\n",
    "5. Determine optimal number of components for each method\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your dimensionality reduction analysis code\n",
    "\n",
    "# 1. Implement different reduction techniques\n",
    "# Your code here...\n",
    "\n",
    "# 2. Compare reconstruction error\n",
    "# Your code here...\n",
    "\n",
    "# 3. Visualize in 2D\n",
    "# Your code here...\n",
    "\n",
    "# 4. Analyze similarity preservation\n",
    "# Your code here...\n",
    "\n",
    "# 5. Determine optimal components\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Extraction Pipeline\n",
    "\n",
    "Let's create a complete feature extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFeaturePipeline:\n",
    "    \"\"\"Complete pipeline for video feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='resnet50', image_size=224, pooling_strategy='mean'):\n",
    "        self.feature_extractor = VideoFeatureExtractor(\n",
    "            model_name=model_name,\n",
    "            image_size=image_size,\n",
    "            pooling_strategy=pooling_strategy\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None\n",
    "        \n",
    "    def fit(self, video_paths, max_frames=30):\n",
    "        \"\"\"Fit the pipeline on training videos\"\"\"\n",
    "        print(\"Fitting feature extraction pipeline...\")\n",
    "        \n",
    "        # Extract features from all videos\n",
    "        all_features = []\n",
    "        for video_path in tqdm(video_paths, desc=\"Extracting features\"):\n",
    "            try:\n",
    "                frames = load_video(video_path, max_frames=max_frames)\n",
    "                features = self.feature_extractor.extract_features(frames)\n",
    "                all_features.append(features.numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "        \n",
    "        # Stack features\n",
    "        features_matrix = np.stack(all_features)\n",
    "        \n",
    "        # Fit scaler\n",
    "        self.scaler.fit(features_matrix)\n",
    "        \n",
    "        # Fit PCA\n",
    "        features_scaled = self.scaler.transform(features_matrix)\n",
    "        self.pca = PCA(n_components=min(100, features_scaled.shape[1]))\n",
    "        self.pca.fit(features_scaled)\n",
    "        \n",
    "        print(f\"Pipeline fitted successfully!\")\n",
    "        print(f\"Original dimension: {features_matrix.shape[1]}\")\n",
    "        print(f\"Reduced dimension: {self.pca.n_components_}\")\n",
    "        print(f\"Explained variance: {self.pca.explained_variance_ratio_.sum():.4f}\")\n",
    "        \n",
    "    def transform(self, video_path, max_frames=30):\n",
    "        \"\"\"Transform a single video to features\"\"\"\n",
    "        # Extract features\n",
    "        frames = load_video(video_path, max_frames=max_frames)\n",
    "        features = self.feature_extractor.extract_features(frames)\n",
    "        \n",
    "        # Scale and reduce\n",
    "        features_scaled = self.scaler.transform(features.numpy().reshape(1, -1))\n",
    "        features_reduced = self.pca.transform(features_scaled)\n",
    "        \n",
    "        return features_reduced.squeeze()\n",
    "\n",
    "# Test the pipeline\n",
    "if video_files:\n",
    "    pipeline = VideoFeaturePipeline()\n",
    "    pipeline.fit(video_files[:5])\n",
    "    \n",
    "    # Transform a test video\n",
    "    test_features = pipeline.transform(video_files[0])\n",
    "    print(f\"\\nTest video features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 6: Pipeline Optimization\n",
    "\n",
    "**Task**: Optimize the feature extraction pipeline for better performance.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement batch processing for faster feature extraction\n",
    "2. Add caching to avoid re-extracting features\n",
    "3. Implement early stopping for PCA based on explained variance\n",
    "4. Add feature selection based on importance scores\n",
    "5. Create a performance comparison between optimized and original pipeline\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your pipeline optimization code\n",
    "\n",
    "# 1. Implement batch processing\n",
    "# Your code here...\n",
    "\n",
    "# 2. Add caching mechanism\n",
    "# Your code here...\n",
    "\n",
    "# 3. Implement early stopping for PCA\n",
    "# Your code here...\n",
    "\n",
    "# 4. Add feature selection\n",
    "# Your code here...\n",
    "\n",
    "# 5. Performance comparison\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ FINAL EXERCISE: Feature Extraction Strategy Report\n",
    "\n",
    "**Task**: Write a comprehensive report on feature extraction strategies.\n",
    "\n",
    "**Requirements**:\n",
    "1. Compare different pre-trained models for video feature extraction\n",
    "2. Analyze the trade-offs between feature dimension and performance\n",
    "3. Recommend optimal pooling strategies for different video types\n",
    "4. Suggest preprocessing techniques for better feature quality\n",
    "5. Propose a complete feature extraction pipeline for production use\n",
    "\n",
    "**Your report here** (write in markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your feature extraction strategy report\n",
    "report = \"\"\"\n",
    "## Feature Extraction Strategy Report\n",
    "\n",
    "### Model Comparison:\n",
    "[Your analysis here]\n",
    "\n",
    "### Dimension vs Performance Trade-offs:\n",
    "[Your analysis here]\n",
    "\n",
    "### Optimal Pooling Strategies:\n",
    "[Your recommendations here]\n",
    "\n",
    "### Preprocessing Techniques:\n",
    "[Your suggestions here]\n",
    "\n",
    "### Production Pipeline:\n",
    "[Your proposal here]\n",
    "\"\"\"\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned:\n",
    "\n",
    "âœ… **Pre-trained Models**: Understanding different architectures and their characteristics\n",
    "âœ… **Feature Extraction**: Converting video frames to numerical representations\n",
    "âœ… **Temporal Pooling**: Combining frame features into video-level features\n",
    "âœ… **Dimensionality Reduction**: Techniques for compressing feature vectors\n",
    "âœ… **Pipeline Design**: Creating efficient feature extraction workflows\n",
    "âœ… **6 Interactive Exercises**: Hands-on analysis requiring deep understanding\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Feature Quality Matters**: The choice of pre-trained model significantly affects feature quality\n",
    "2. **Temporal Information**: How we combine frame features affects the final video representation\n",
    "3. **Dimensionality Trade-offs**: More features aren't always better - compression can improve performance\n",
    "4. **Pipeline Efficiency**: Batch processing and caching are crucial for large-scale applications\n",
    "5. **Model Selection**: Different models work better for different types of video content\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll learn about **Model Architecture** - how to design neural networks for video similarity learning.\n",
    "\n",
    "---\n",
    "\n",
    "**Questions to think about:**\n",
    "- Which pre-trained model would work best for your specific video domain?\n",
    "- How would you handle videos of very different lengths?\n",
    "- What preprocessing steps would improve feature quality?\n",
    "- How would you design a feature extraction pipeline for real-time applications?\n",
    "- What metrics would you use to evaluate feature quality?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}