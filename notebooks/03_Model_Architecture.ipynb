 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Architecture\n",
    "\n",
    "In this notebook, we'll design and implement neural network architectures for video similarity learning.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand different similarity learning architectures\n",
    "- Implement Siamese networks and triplet networks\n",
    "- Design custom loss functions for similarity learning\n",
    "- Compare different architectural choices\n",
    "- **Complete 5 hands-on exercises** requiring architectural design\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Siamese Networks**: Neural networks that learn to compare two inputs and determine their similarity.\n",
    "\n",
    "**Triplet Networks**: Networks that learn from triplets of examples (anchor, positive, negative).\n",
    "\n",
    "**Contrastive Loss**: Loss function that pushes similar pairs closer and dissimilar pairs apart.\n",
    "\n",
    "**Triplet Loss**: Loss function that ensures positive examples are closer to anchor than negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our utilities\n",
    "from utils.model_utils import VideoFeatureExtractor, get_model_summary\n",
    "from utils.video_utils import load_video, create_frame_transforms\n",
    "from utils.data_utils import VideoDataset\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Similarity Learning\n",
    "\n",
    "Let's start by understanding the fundamental concepts of similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity learning concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Siamese Network\n",
    "axes[0].text(0.5, 0.5, 'Siamese Network\\n\\nInput A â†’ Encoder â†’ Features A\\nInput B â†’ Encoder â†’ Features B\\n\\nCompare Features', \n",
    "             ha='center', va='center', transform=axes[0].transAxes, fontsize=12)\n",
    "axes[0].set_title('Siamese Network')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Triplet Network\n",
    "axes[1].text(0.5, 0.5, 'Triplet Network\\n\\nAnchor â†’ Encoder â†’ Features A\\nPositive â†’ Encoder â†’ Features P\\nNegative â†’ Encoder â†’ Features N\\n\\nLearn: d(A,P) < d(A,N)', \n",
    "             ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "axes[1].set_title('Triplet Network')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Contrastive Learning\n",
    "axes[2].text(0.5, 0.5, 'Contrastive Learning\\n\\nSimilar pairs: Push closer\\nDifferent pairs: Push apart\\n\\nLearn meaningful representations', \n",
    "             ha='center', va='center', transform=axes[2].transAxes, fontsize=12)\n",
    "axes[2].set_title('Contrastive Learning')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Siamese Network Implementation\n",
    "\n",
    "Let's implement a basic Siamese network for video similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoSiameseNetwork(nn.Module):\n",
    "    \"\"\"Basic Siamese network for video similarity learning\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=2048, embedding_dim=128):\n",
    "        super(VideoSiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Feature extractor (we'll use pre-extracted features)\n",
    "        self.feature_extractor = VideoFeatureExtractor(\n",
    "            model_name='resnet50',\n",
    "            image_size=224,\n",
    "            pooling_strategy='mean'\n",
    "        )\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Similarity layer\n",
    "        self.similarity_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, video_frames):\n",
    "        \"\"\"Extract features and create embedding for one video\"\"\"\n",
    "        features = self.feature_extractor.extract_features(video_frames)\n",
    "        embedding = self.embedding(features)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, video1_frames, video2_frames):\n",
    "        \"\"\"Forward pass for two videos\"\"\"\n",
    "        # Extract embeddings\n",
    "        embedding1 = self.forward_one(video1_frames)\n",
    "        embedding2 = self.forward_one(video2_frames)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined = torch.cat([embedding1, embedding2], dim=1)\n",
    "        \n",
    "        # Predict similarity\n",
    "        similarity = self.similarity_layer(combined)\n",
    "        \n",
    "        return similarity.squeeze()\n",
    "\n",
    "# Create model\n",
    "model = VideoSiameseNetwork()\n",
    "print(\"Siamese network created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 1: Siamese Network Analysis\n",
    "\n",
    "**Task**: Analyze and improve the Siamese network architecture.\n",
    "\n",
    "**Requirements**:\n",
    "1. Calculate the number of parameters in each layer of the network\n",
    "2. Implement a different similarity function (cosine similarity, Euclidean distance)\n",
    "3. Add batch normalization to improve training stability\n",
    "4. Create a visualization of the network architecture\n",
    "5. Suggest architectural improvements for better performance\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your Siamese network analysis code\n",
    "\n",
    "# 1. Calculate parameters per layer\n",
    "# Your code here...\n",
    "\n",
    "# 2. Implement different similarity functions\n",
    "# Your code here...\n",
    "\n",
    "# 3. Add batch normalization\n",
    "# Your code here...\n",
    "\n",
    "# 4. Create architecture visualization\n",
    "# Your code here...\n",
    "\n",
    "# 5. Suggest improvements\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Triplet Network Implementation\n",
    "\n",
    "Now let's implement a triplet network for more effective similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTripletNetwork(nn.Module):\n",
    "    \"\"\"Triplet network for video similarity learning\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=2048, embedding_dim=128):\n",
    "        super(VideoTripletNetwork, self).__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feature_extractor = VideoFeatureExtractor(\n",
    "            model_name='resnet50',\n",
    "            image_size=224,\n",
    "            pooling_strategy='mean'\n",
    "        )\n",
    "        \n",
    "        # Embedding network (shared across anchor, positive, negative)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, video_frames):\n",
    "        \"\"\"Extract embedding for one video\"\"\"\n",
    "        features = self.feature_extractor.extract_features(video_frames)\n",
    "        embedding = self.embedding(features)\n",
    "        # L2 normalize embeddings\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, anchor_frames, positive_frames, negative_frames):\n",
    "        \"\"\"Forward pass for triplet (anchor, positive, negative)\"\"\"\n",
    "        anchor_embedding = self.forward_one(anchor_frames)\n",
    "        positive_embedding = self.forward_one(positive_frames)\n",
    "        negative_embedding = self.forward_one(negative_frames)\n",
    "        \n",
    "        return anchor_embedding, positive_embedding, negative_embedding\n",
    "\n",
    "# Create triplet network\n",
    "triplet_model = VideoTripletNetwork()\n",
    "print(\"Triplet network created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in triplet_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions for Similarity Learning\n",
    "\n",
    "Let's implement different loss functions for similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive loss for Siamese networks\"\"\"\n",
    "    \n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding1, embedding2: Embeddings of two videos\n",
    "            label: 1 if similar, 0 if different\n",
    "        \"\"\"\n",
    "        # Calculate Euclidean distance\n",
    "        distance = F.pairwise_distance(embedding1, embedding2)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        loss_similar = label * torch.pow(distance, 2)\n",
    "        loss_different = (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        \n",
    "        loss = torch.mean(loss_similar + loss_different)\n",
    "        return loss\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss for triplet networks\"\"\"\n",
    "    \n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            anchor, positive, negative: Embeddings of triplet\n",
    "        \"\"\"\n",
    "        # Calculate distances\n",
    "        pos_distance = F.pairwise_distance(anchor, positive)\n",
    "        neg_distance = F.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        # Triplet loss\n",
    "        loss = torch.clamp(pos_distance - neg_distance + self.margin, min=0.0)\n",
    "        loss = torch.mean(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Test loss functions\n",
    "print(\"Loss functions created successfully!\")\n",
    "print(\"\\nContrastive Loss:\")\n",
    "print(\"- Pushes similar pairs closer together\")\n",
    "print(\"- Pushes different pairs apart (beyond margin)\")\n",
    "print(\"\\nTriplet Loss:\")\n",
    "print(\"- Ensures positive is closer to anchor than negative\")\n",
    "print(\"- Uses margin to create separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 2: Loss Function Analysis\n",
    "\n",
    "**Task**: Analyze and compare different loss functions for similarity learning.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement additional loss functions (N-pair loss, angular loss)\n",
    "2. Create a function to visualize loss landscapes\n",
    "3. Compare the behavior of different loss functions on sample data\n",
    "4. Analyze the impact of margin values on training\n",
    "5. Suggest optimal loss functions for different scenarios\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your loss function analysis code\n",
    "\n",
    "# 1. Implement additional loss functions\n",
    "# Your code here...\n",
    "\n",
    "# 2. Create loss landscape visualization\n",
    "# Your code here...\n",
    "\n",
    "# 3. Compare loss behaviors\n",
    "# Your code here...\n",
    "\n",
    "# 4. Analyze margin impact\n",
    "# Your code here...\n",
    "\n",
    "# 5. Suggest optimal losses\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Architectures\n",
    "\n",
    "Let's implement more advanced architectures for video similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBasedSiamese(nn.Module):\n",
    "    \"\"\"Siamese network with attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=2048, embedding_dim=128, num_heads=8):\n",
    "        super(AttentionBasedSiamese, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = VideoFeatureExtractor(\n",
    "            model_name='resnet50',\n",
    "            image_size=224,\n",
    "            pooling_strategy='none'  # Get frame-level features\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=feature_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Similarity layer\n",
    "        self.similarity_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, video_frames):\n",
    "        \"\"\"Extract features with attention\"\"\"\n",
    "        # Get frame-level features\n",
    "        frame_features = self.feature_extractor.extract_frame_features(video_frames)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended_features, _ = self.attention(frame_features, frame_features, frame_features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        video_features = torch.mean(attended_features, dim=1)\n",
    "        \n",
    "        # Create embedding\n",
    "        embedding = self.embedding(video_features)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, video1_frames, video2_frames):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        embedding1 = self.forward_one(video1_frames)\n",
    "        embedding2 = self.forward_one(video2_frames)\n",
    "        \n",
    "        combined = torch.cat([embedding1, embedding2], dim=1)\n",
    "        similarity = self.similarity_layer(combined)\n",
    "        \n",
    "        return similarity.squeeze()\n",
    "\n",
    "# Create attention-based model\n",
    "attention_model = AttentionBasedSiamese()\n",
    "print(\"Attention-based Siamese network created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in attention_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 3: Advanced Architecture Design\n",
    "\n",
    "**Task**: Design and implement advanced architectures for video similarity learning.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement a temporal convolutional network (TCN) for video processing\n",
    "2. Design a hierarchical attention mechanism\n",
    "3. Create a multi-scale feature fusion architecture\n",
    "4. Implement a graph neural network for video similarity\n",
    "5. Compare the computational complexity of different architectures\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your advanced architecture design code\n",
    "\n",
    "# 1. Implement TCN\n",
    "# Your code here...\n",
    "\n",
    "# 2. Design hierarchical attention\n",
    "# Your code here...\n",
    "\n",
    "# 3. Create multi-scale fusion\n",
    "# Your code here...\n",
    "\n",
    "# 4. Implement GNN\n",
    "# Your code here...\n",
    "\n",
    "# 5. Compare complexity\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison\n",
    "\n",
    "Let's create functions to evaluate and compare different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluate a similarity learning model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    similarities = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            video1, video2, label = batch\n",
    "            \n",
    "            # Move to device\n",
    "            video1 = video1.to(device)\n",
    "            video2 = video2.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            pred = model(video1, video2)\n",
    "            \n",
    "            similarities.extend(pred.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # ROC AUC\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    auc = roc_auc_score(labels, similarities)\n",
    "    \n",
    "    # Precision-Recall\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    ap = average_precision_score(labels, similarities)\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'ap': ap,\n",
    "        'similarities': similarities,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def compare_architectures(models, test_loader, device='cpu'):\n",
    "    \"\"\"Compare multiple model architectures\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        results[name] = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ROC curves\n",
    "    for name, result in results.items():\n",
    "        fpr, tpr, _ = roc_curve(result['labels'], result['similarities'])\n",
    "        axes[0].plot(fpr, tpr, label=f'{name} (AUC={result[\"auc\"]:.3f})')\n",
    "    \n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curves\n",
    "    for name, result in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(result['labels'], result['similarities'])\n",
    "        axes[1].plot(recall, precision, label=f'{name} (AP={result[\"ap\"]:.3f})')\n",
    "    \n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title('Precision-Recall Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Model evaluation functions created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 4: Model Evaluation and Analysis\n",
    "\n",
    "**Task**: Evaluate and analyze different model architectures.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create synthetic test data for model evaluation\n",
    "2. Implement additional evaluation metrics (F1-score, confusion matrix)\n",
    "3. Analyze model performance on different types of video pairs\n",
    "4. Create a model complexity vs performance trade-off analysis\n",
    "5. Suggest ensemble methods for improved performance\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your model evaluation and analysis code\n",
    "\n",
    "# 1. Create synthetic test data\n",
    "# Your code here...\n",
    "\n",
    "# 2. Implement additional metrics\n",
    "# Your code here...\n",
    "\n",
    "# 3. Analyze performance on different video types\n",
    "# Your code here...\n",
    "\n",
    "# 4. Complexity vs performance analysis\n",
    "# Your code here...\n",
    "\n",
    "# 5. Suggest ensemble methods\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Design Patterns\n",
    "\n",
    "Let's explore common design patterns for video similarity learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design pattern: Feature fusion\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    \"\"\"Module for fusing different types of features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dims, fusion_dim=256):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        \n",
    "        self.feature_dims = feature_dims\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # Individual feature projections\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Linear(dim, fusion_dim) for dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * len(feature_dims), fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, features_list):\n",
    "        \"\"\"Fuse multiple feature vectors\"\"\"\n",
    "        # Project each feature to same dimension\n",
    "        projected_features = []\n",
    "        for i, features in enumerate(features_list):\n",
    "            projected = self.projections[i](features)\n",
    "            projected_features.append(projected)\n",
    "        \n",
    "        # Concatenate and fuse\n",
    "        concatenated = torch.cat(projected_features, dim=1)\n",
    "        fused = self.fusion(concatenated)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "# Design pattern: Multi-scale processing\n",
    "class MultiScaleProcessor(nn.Module):\n",
    "    \"\"\"Process video at multiple temporal scales\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dim, scales=[1, 2, 4]):\n",
    "        super(MultiScaleProcessor, self).__init__()\n",
    "        \n",
    "        self.scales = scales\n",
    "        self.processors = nn.ModuleList([\n",
    "            nn.Conv1d(base_dim, base_dim, kernel_size=scale, stride=scale)\n",
    "            for scale in scales\n",
    "        ])\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Linear(base_dim * len(scales), base_dim)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \"\"\"Process features at multiple scales\"\"\"\n",
    "        # features shape: (batch, time, dim)\n",
    "        features = features.transpose(1, 2)  # (batch, dim, time)\n",
    "        \n",
    "        multi_scale_features = []\n",
    "        for processor in self.processors:\n",
    "            processed = processor(features)\n",
    "            # Global average pooling\n",
    "            pooled = torch.mean(processed, dim=2)\n",
    "            multi_scale_features.append(pooled)\n",
    "        \n",
    "        # Fuse multi-scale features\n",
    "        concatenated = torch.cat(multi_scale_features, dim=1)\n",
    "        fused = self.fusion(concatenated)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "print(\"Design pattern modules created successfully!\")\n",
    "print(\"\\nFeatureFusionModule: Combines different feature types\")\n",
    "print(\"MultiScaleProcessor: Processes video at multiple temporal scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ EXERCISE 5: Architecture Design Patterns\n",
    "\n",
    "**Task**: Implement and analyze different architecture design patterns.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement a residual connection pattern for video processing\n",
    "2. Design a skip connection architecture for multi-scale features\n",
    "3. Create a bottleneck design for efficient processing\n",
    "4. Implement a pyramid network for hierarchical feature extraction\n",
    "5. Compare the effectiveness of different design patterns\n",
    "\n",
    "**Your code here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your architecture design patterns code\n",
    "\n",
    "# 1. Implement residual connections\n",
    "# Your code here...\n",
    "\n",
    "# 2. Design skip connections\n",
    "# Your code here...\n",
    "\n",
    "# 3. Create bottleneck design\n",
    "# Your code here...\n",
    "\n",
    "# 4. Implement pyramid network\n",
    "# Your code here...\n",
    "\n",
    "# 5. Compare design patterns\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ FINAL EXERCISE: Architecture Design Report\n",
    "\n",
    "**Task**: Write a comprehensive report on architecture design for video similarity learning.\n",
    "\n",
    "**Requirements**:\n",
    "1. Compare Siamese vs Triplet networks for different scenarios\n",
    "2. Analyze the impact of attention mechanisms on performance\n",
    "3. Recommend optimal architectures for different video types\n",
    "4. Suggest architectural improvements for real-time applications\n",
    "5. Propose a complete architecture design methodology\n",
    "\n",
    "**Your report here** (write in markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your architecture design report\n",
    "report = \"\"\"\n",
    "## Architecture Design Report\n",
    "\n",
    "### Siamese vs Triplet Networks:\n",
    "[Your analysis here]\n",
    "\n",
    "### Attention Mechanism Impact:\n",
    "[Your analysis here]\n",
    "\n",
    "### Optimal Architectures:\n",
    "[Your recommendations here]\n",
    "\n",
    "### Real-time Improvements:\n",
    "[Your suggestions here]\n",
    "\n",
    "### Design Methodology:\n",
    "[Your proposal here]\n",
    "\"\"\"\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned:\n",
    "\n",
    "âœ… **Similarity Learning**: Understanding Siamese and Triplet networks\n",
    "âœ… **Loss Functions**: Implementing contrastive and triplet losses\n",
    "âœ… **Advanced Architectures**: Attention mechanisms and feature fusion\n",
    "âœ… **Design Patterns**: Common patterns for video similarity learning\n",
    "âœ… **Model Evaluation**: Comparing different architectures\n",
    "âœ… **5 Interactive Exercises**: Hands-on architectural design\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Architecture Choice Matters**: Different architectures work better for different scenarios\n",
    "2. **Loss Function Design**: The choice of loss function significantly impacts learning\n",
    "3. **Attention Mechanisms**: Can improve performance by focusing on important parts\n",
    "4. **Design Patterns**: Reusable components can speed up development\n",
    "5. **Evaluation is Key**: Proper evaluation helps choose the best architecture\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll learn about **Training Setup** - how to prepare data and configure training parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**Questions to think about:**\n",
    "- Which architecture would work best for your specific video domain?\n",
    "- How would you handle videos of very different lengths?\n",
    "- What loss function would be most appropriate for your use case?\n",
    "- How would you design an architecture for real-time video similarity?\n",
    "- What evaluation metrics are most important for your application?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}